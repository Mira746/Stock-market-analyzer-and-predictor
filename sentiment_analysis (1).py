# -*- coding: utf-8 -*-
"""Sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBqTW41P17EDCskSRwnbAUWeg86JFkGv
"""

! pip install kaggle

"""Upload kaggle.json file"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Importing twitter sentiment dataset using API"""

!kaggle datasets download -d equinxx/stock-tweets-for-sentiment-analysis-and-prediction

!unzip "archive (2).zip"

"""Importing dependencies"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df=pd.read_csv('/content/stock_tweets.csv',encoding='latin-1')

df.columns

df.head()

!pip install keybert
!pip install sentence-transformers
!pip install torch torchvision torchaudio
!pip install pandas numpy matplotlib seaborn
!pip install wordcloud plotly

!pip install torch torchvision torchaudio --force-reinstall

import torch
! pip install keybert
! pip install sentence-transformers  # required dependency

"""Extracting Keywords using KeyBERT"""

import pandas as pd
import re
from keybert import KeyBERT

# Load your dataset
df = pd.read_csv('stock_tweets.csv')  # adjust path if needed
print(df.head())

# Build regex pattern to match finance-related tweets
pattern = r'\b(stock|shares?|portfolio|trading)\b.*\b(buy|sell|gm|price|market|share|invest|portfolio|trade)\b|\b(buy|sell|gm|price|market|share|invest|portfolio|trade)\b.*\b(stock|shares?|portfolio|trading)\b'

# Filter on the 'Tweet' column
filtered_df = df[df['Tweet'].str.contains(pattern, flags=re.IGNORECASE, na=False, regex=True)]

print(f"Filtered tweets count: {len(filtered_df)}")

# Initialize KeyBERT
kw_model = KeyBERT(model='all-MiniLM-L6-v2')

# Run extraction
for i, tweet_text in enumerate(filtered_df['Tweet']):
    keywords = kw_model.extract_keywords(tweet_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)
    print(f"\nTWEET {i+1}: {tweet_text}")
    print(f"KEYWORDS: {keywords}")

    # Optional break after 100 to limit output
    if i > 100:
        break

pip install transformers torch

"""Carrying out Sentiment analysis using FinBERT"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

# Load FinBERT
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")

def classify_finbert(Tweet):
    inputs = tokenizer(Tweet, return_tensors="pt", truncation=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    probs = F.softmax(logits, dim=-1)
    predicted_class = torch.argmax(probs).item()
    labels = ['neutral', 'positive', 'negative']
    return labels[predicted_class], probs[0].tolist()

for i, tweet_text in enumerate(filtered_df['Tweet']):
    sentiment, probs = classify_finbert(tweet_text)
    print(f"\nTWEET {i+1}: {tweet_text}")
    print(f"FINBERT SENTIMENT: {sentiment} (probs: {probs})")

sentiments = []
probs_list = []

for Tweet in filtered_df['Tweet']:
    sentiment, probs = classify_finbert(Tweet)
    sentiments.append(sentiment)
    probs_list.append(probs)

filtered_df['sentiment'] = sentiments
filtered_df['probabilities'] = probs_list

print(filtered_df.head())
print(filtered_df['sentiment'].value_counts())

print(filtered_df['Date'].min(), filtered_df['Date'].max())

import yfinance as yf

gm_stock = yf.download("GM", start="2021-09-29", end="2022-09-30")

print(gm_stock.head())
print(gm_stock.tail())

# filtered_df['Date'] = pd.to_datetime(filtered_df['Date']).dt.date  # convert to date only (no time)

gm_stock = gm_stock.reset_index()
gm_stock['Date'] = pd.to_datetime(gm_stock['Date']).dt.date

merged_df = pd.merge(filtered_df, gm_stock, on='Date', how='inner')  # inner join to keep only matching dates

print(f"Number of records after merge: {len(merged_df)}")
print(merged_df.head())

print(merged_df.head())

print(merged_df.columns)

print(merged_df.columns.tolist())

df.columns = df.columns.str.strip()

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Encode sentiment as numeric
sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
merged_df['sentiment'] = merged_df['sentiment'].map(sentiment_map)

# --- 1. Create lag features ---
# Let's create 3-day lag features for stock prices and sentiment
lags = 3
for lag in range(1, lags+1):
    merged_df[f'Close_GM_lag{lag}'] = merged_df['Close_GM'].shift(lag)
    merged_df[f'Sentiment_lag{lag}'] = merged_df['sentiment'].shift(lag)


# Drop rows with NaN after lagging
merged_df = merged_df.dropna().reset_index(drop=True)


# --- 2. Features & Target ---
# We’ll predict next-day Close price
feature_cols = [
    'Close_GM_lag1', 'Sentiment_lag1',
    'Close_GM_lag2', 'Sentiment_lag2',
    'Close_GM_lag3', 'Sentiment_lag3'
]
target_col = 'Close_GM'

X = merged_df[feature_cols]
y = merged_df[target_col]

# --- 3. Train/Test Split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False  # no random shuffle for temporal order
)

# --- 4. Train Random Forest ---
rf = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    max_depth=10
)
rf.fit(X_train, y_train)

# --- 5. Predictions ---
y_pred = rf.predict(X_test)

# --- 6. Metrics ---
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("Random Forest R²:", r2_score(y_test, y_pred))

# --- 7. Feature Importance ---
importances = rf.feature_importances_
feature_names = X.columns
sorted_idx = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.barh(range(len(importances)), importances[sorted_idx], align='center')
plt.yticks(range(len(importances)), feature_names[sorted_idx])
plt.gca().invert_yaxis()
plt.title("Feature Importances - Random Forest")
plt.show()

from sklearn.metrics import accuracy_score

# Assume your model is already trained: model.fit(X_train, y_train)

# --- Hardcoded test data ---
# Make sure feature order matches your training features
X_test_manual = [
    [120.5, 1, 1500000],   # example row: [Close_GM_lag1, Sentiment_lag1, Volume_GM]
    [119.2, -1, 1600000],
    [121.0, 0, 1400000]
]
y_test_manual = [1, 0, 1]  # true labels

# --- Predict and check accuracy ---
y_pred_manual = model.predict(X_test_manual)
acc_manual = accuracy_score(y_test_manual, y_pred_manual)

print("Manual test accuracy:", acc_manual)
print("Predictions:", y_pred_manual.tolist())
print("Actual:", y_test_manual)

# filtered_df['Date'] = pd.to_datetime(filtered_df['Date']).dt.date  # convert to date only (no time)

# gm_stock = gm_stock.reset_index() # Remove this line
gm_stock['Date'] = pd.to_datetime(gm_stock['Date']).dt.date

merged_df = pd.merge(filtered_df, gm_stock, on='Date', how='inner')  # inner join to keep only matching dates

print(f"Number of records after merge: {len(merged_df)}")
print(merged_df.head())

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Encode sentiment as numeric
sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
merged_df['sentiment'] = merged_df['sentiment'].map(sentiment_map)

# --- 1. Create lag features ---
# Let's create 3-day lag features for stock prices and sentiment
lags = 3
for lag in range(1, lags+1):
    merged_df[f'Close_GM_lag{lag}'] = merged_df['Close_GM'].shift(lag)
    merged_df[f'Sentiment_lag{lag}'] = merged_df['sentiment'].shift(lag)

print(f"Shape before dropping NaNs: {merged_df.shape}")

# Drop rows with NaN after lagging
merged_df = merged_df.dropna().reset_index(drop=True)

print(f"Shape after dropping NaNs: {merged_df.shape}")

# --- 2. Features & Target ---
# We’ll predict next-day Close price
feature_cols = [
    'Close_GM_lag1', 'Sentiment_lag1',
    'Close_GM_lag2', 'Sentiment_lag2',
    'Close_GM_lag3', 'Sentiment_lag3'
]
target_col = 'Close_GM'

X = merged_df[feature_cols]
y = merged_df[target_col]

print(f"Shape of X: {X.shape}")
print(f"Shape of y: {y.shape}")


# --- 3. Train/Test Split ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False  # no random shuffle for temporal order
)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")


# --- 4. Train Random Forest ---
rf = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    max_depth=10
)
rf.fit(X_train, y_train)

# --- 5. Predictions ---
y_pred = rf.predict(X_test)

# --- 6. Metrics ---
print("Random Forest RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("Random Forest R²:", r2_score(y_test, y_pred))

# --- 7. Feature Importance ---
importances = rf.feature_importances_
feature_names = X.columns
sorted_idx = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.barh(range(len(importances)), importances[sorted_idx], align='center')
plt.yticks(range(len(importances)), feature_names[sorted_idx])
plt.gca().invert_yaxis()
plt.title("Feature Importances - Random Forest")
plt.show()

"""## Model Results

The Random Forest model was trained to predict the next day's closing price of GM stock based on lagged closing prices and the sentiment of relevant tweets.

**Performance Metrics:**
- **RMSE (Root Mean Squared Error):** Measures the standard deviation of the residuals (prediction errors). A lower RMSE indicates a better fit of the model to the data.
- **R² (R-squared):** Represents the proportion of the variance in the dependent variable (stock price) that is predictable from the independent variables (lagged prices and sentiment). A higher R² indicates a better fit.

**Feature Importances:**
The bar chart above shows the importance of each feature in predicting the stock price according to the Random Forest model. Features with higher importance scores have a greater impact on the model's predictions. This can help understand which factors (past stock prices or sentiment) are most influential in the model's predictions.

Based on the results, you can interpret:
* How well the model performs based on the RMSE and R² values.
* Which lagged stock prices and sentiment scores are most important for predicting the next day's closing price.

# Task
Analyze the relationship between sentiment data and stock prices by performing the following steps: 1. Aggregate sentiment data by date and calculate the average sentiment for each day. 2. Merge the aggregated sentiment data with the stock price data. 3. Create lagged features for both stock prices and sentiment. 4. Prepare the data for modeling by separating features and target variables. 5. Train a Random Forest Regressor model to predict stock prices based on the features. 6. Evaluate the model's performance using RMSE and R². 7. Analyze the feature importances to understand the influence of sentiment and lagged stock prices on predictions. Use the dataframes `filtered_df` and `gm_stock`.

## Aggregate sentiment

### Subtask:
Group `filtered_df` by date and calculate the average sentiment for each day.

**Reasoning**:
Convert the 'Date' column to date objects, group by date, and calculate the mean sentiment for each day.
"""

filtered_df['Date'] = pd.to_datetime(filtered_df['Date']).dt.date
daily_sentiment = filtered_df.groupby('Date')['sentiment'].mean().reset_index()
daily_sentiment.columns = ['Date', 'Average_Sentiment']
display(daily_sentiment.head())

"""**Reasoning**:
The error indicates that the 'sentiment' column is not numeric, which is required for calculating the mean. I need to convert the sentiment labels ('neutral', 'positive', 'negative') into numerical values before calculating the mean.


"""

sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
filtered_df['sentiment_numeric'] = filtered_df['sentiment'].map(sentiment_map)
filtered_df['Date'] = pd.to_datetime(filtered_df['Date']).dt.date
daily_sentiment = filtered_df.groupby('Date')['sentiment_numeric'].mean().reset_index()
daily_sentiment.columns = ['Date', 'Average_Sentiment']
display(daily_sentiment.head())

"""## Merge with stock data

### Subtask:
Merge the daily aggregated sentiment with the `gm_stock` dataframe based on the date.

**Reasoning**:
Convert the 'Date' column in `gm_stock` to datetime objects and extract the date, then merge `daily_sentiment` and `gm_stock` on the date column, and finally display the head and shape of the merged dataframe.
"""

gm_stock['Date'] = pd.to_datetime(gm_stock['Date']).dt.date
merged_df = pd.merge(daily_sentiment, gm_stock, on='Date', how='inner')
display(merged_df.head())
print(f"Shape of merged_df: {merged_df.shape}")

"""## Create lag features

### Subtask:
Generate lagged features for both the daily closing price and the daily average sentiment.

**Reasoning**:
Generate 3-day lag features for both the daily closing price and the daily average sentiment.
"""

lags = 3
for lag in range(1, lags + 1):
    merged_df[f'Close_GM_lag{lag}'] = merged_df['Close_GM'].shift(lag)
    merged_df[f'Average_Sentiment_lag{lag}'] = merged_df['Average_Sentiment'].shift(lag)

display(merged_df.head())

"""## Prepare data for modeling

### Subtask:
Drop rows with missing values resulting from the lagging and separate the features (X) and target (y) variables.

**Reasoning**:
Drop rows with missing values resulting from the lagging and separate the features (X) and target (y) variables as per the instructions.
"""

merged_df = merged_df.dropna().reset_index(drop=True)

feature_cols = [
    'Close_GM_lag1', 'Average_Sentiment_lag1',
    'Close_GM_lag2', 'Average_Sentiment_lag2',
    'Close_GM_lag3', 'Average_Sentiment_lag3'
]
target_col = 'Close_GM'

X = merged_df[feature_cols]
y = merged_df[target_col]

print(f"Shape of X after dropping NaNs: {X.shape}")
print(f"Shape of y after dropping NaNs: {y.shape}")

"""## Train and evaluate model

### Subtask:
Split the data into training and testing sets and train a Random Forest Regressor model. Evaluate the model's performance using RMSE and R².

**Reasoning**:
Split the data into training and testing sets and train a Random Forest Regressor model to predict stock prices based on the features. Evaluate the model's performance using RMSE and R² and print the metrics.
"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

rf = RandomForestRegressor(
    n_estimators=200,
    random_state=42,
    max_depth=10
)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("Random Forest RMSE:", rmse)
print("Random Forest R²:", r2)

"""**Reasoning**:
The previous step successfully trained the model and calculated the evaluation metrics. The next step is to analyze the feature importances as requested in the original task.


"""

importances = rf.feature_importances_
feature_names = X.columns
sorted_idx = np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.barh(range(len(importances)), importances[sorted_idx], align='center')
plt.yticks(range(len(importances)), feature_names[sorted_idx])
plt.gca().invert_yaxis()
plt.title("Feature Importances - Random Forest")
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The Random Forest Regressor model achieved an RMSE of approximately 1.12 and an R² score of approximately 0.73 on the test set, indicating it explains about 73% of the variance in stock prices with an average prediction error of around \$1.12.
*   Lagged stock prices and lagged average sentiment were used as features to predict the current day's closing stock price.
*   The feature importance analysis indicated that lagged stock prices were more influential in predicting the current stock price than lagged average sentiment.

### Insights or Next Steps

*   While the model shows a reasonable R² score, exploring other models or incorporating additional features (e.g., trading volume, technical indicators) could potentially improve prediction accuracy.
*   Further analysis of the feature importances could involve examining the specific lag periods that are most influential for both stock price and sentiment to understand the temporal relationships better.

## Summary:

### Data Analysis Key Findings

* The Random Forest Regressor model achieved an RMSE of approximately {{rmse}} and an R² score of approximately {{r2}} on the test set, indicating it explains about {{r2*100:.0f}}% of the variance in stock prices with an average prediction error of around \${{rmse:.2f}}.
* Lagged stock prices and lagged average sentiment were used as features to predict the current day's closing stock price.
* The feature importance analysis indicated that lagged stock prices were more influential in predicting the current stock price than lagged average sentiment.

### Insights or Next Steps

* While the model shows a reasonable R² score, exploring other models or incorporating additional features (e.g., trading volume, technical indicators) could potentially improve prediction accuracy.
* Further analysis of the feature importances could involve examining the specific lag periods that are most influential for both stock price and sentiment to understand the temporal relationships better.
"""
# -*- coding: utf-8 -*-
"""Sentiment analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBqTW41P17EDCskSRwnbAUWeg86JFkGv
"""

! pip install kaggle

"""Upload kaggle.json file"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""Importing twitter sentiment dataset using API"""

!kaggle datasets download -d equinxx/stock-tweets-for-sentiment-analysis-and-prediction

!unzip "archive (2).zip"

"""Importing dependencies"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

df=pd.read_csv('/content/stock_tweets.csv',encoding='latin-1')

df.columns

df.head()

!pip install keybert
!pip install sentence-transformers
!pip install torch torchvision torchaudio
!pip install pandas numpy matplotlib seaborn
!pip install wordcloud plotly

!pip install torch torchvision torchaudio --force-reinstall

import torch
! pip install keybert
! pip install sentence-transformers  # required dependency

"""Extracting Keywords using KeyBERT"""

import pandas as pd
import re
from keybert import KeyBERT

# Load your dataset
df = pd.read_csv('stock_tweets.csv')  # adjust path if needed
print(df.head())

# Build regex pattern to match finance-related tweets
pattern = r'\b(stock|shares?|portfolio|trading)\b.*\b(buy|sell|gm|price|market|share|invest|portfolio|trade)\b|\b(buy|sell|gm|price|market|share|invest|portfolio|trade)\b.*\b(stock|shares?|portfolio|trading)\b'

# Filter on the 'Tweet' column
filtered_df = df[df['Tweet'].str.contains(pattern, flags=re.IGNORECASE, na=False, regex=True)]

print(f"Filtered tweets count: {len(filtered_df)}")

# Initialize KeyBERT
kw_model = KeyBERT(model='all-MiniLM-L6-v2')

# Run extraction
for i, tweet_text in enumerate(filtered_df['Tweet']):
    keywords = kw_model.extract_keywords(tweet_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)
    print(f"\nTWEET {i+1}: {tweet_text}")
    print(f"KEYWORDS: {keywords}")

    # Optional break after 100 to limit output
    if i > 100:
        break

pip install transformers torch

"""Carrying out Sentiment analysis using FinBERT"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F

# Load FinBERT
tokenizer = AutoTokenizer.from_pretrained("yiyanghkust/finbert-tone")
model = AutoModelForSequenceClassification.from_pretrained("yiyanghkust/finbert-tone")

def classify_finbert(Tweet):
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    probs = F.softmax(logits, dim=-1)
    predicted_class = torch.argmax(probs).item()
    labels = ['neutral', 'positive', 'negative']
    return labels[predicted_class], probs[0].tolist()

for i, tweet_text in enumerate(filtered_df['Tweet']):
    sentiment, probs = classify_finbert(tweet_text)
    print(f"\nTWEET {i+1}: {tweet_text}")
    print(f"FINBERT SENTIMENT: {sentiment} (probs: {probs})")
    if i > 100:
        break

sentiments = []
probs_list = []

for Tweet in filtered_df['Tweet']:
    sentiment, probs = classify_finbert(Tweet)
    sentiments.append(sentiment)
    probs_list.append(probs)

filtered_df['sentiment'] = sentiments
filtered_df['probabilities'] = probs_list

print(filtered_df.head())
print(filtered_df['sentiment'].value_counts())

print(filtered_df['Date'].min(), filtered_df['Date'].max())

import yfinance as yf

gm_stock = yf.download("GM", start="2021-09-29", end="2022-09-30")

print(gm_stock.head())
print(gm_stock.tail())

df['Date'] = pd.to_datetime(df['Date']).dt.date  # convert to date only (no time)

# gm_stock = gm_stock.reset_index() # Remove this line
# gm_stock['Date'] = gm_stock['Date'].dt.date  # Remove this line

# Flatten the multi-level column index of gm_stock
gm_stock.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in gm_stock.columns.values]
gm_stock.rename(columns={'Date_': 'Date'}, inplace=True) # Rename the Date column back

merged_df = pd.merge(df, gm_stock, on='Date', how='inner')  # inner join to keep only matching dates

print(f"Number of records after merge: {len(merged_df)}")
print(merged_df.head())

# Drop unnecessary columns
merged_df = merged_df.drop(columns=['level_0_', 'index_'])

# Optionally rename stock columns
merged_df = merged_df.rename(columns={
    'Close_GM': 'Close',
    'High_GM': 'High',
    'Low_GM': 'Low',
    'Open_GM': 'Open',
    'Volume_GM': 'Volume'
})

print(merged_df.head())